---
title: "Individual assignment"
author: "Bernardo"
date: "`r Sys.Date()`"
output: html_document
---

# Individual assignment

## LCS - Lettuce forecasting

## Live session notes

-   There are three months of data

-   Data from 4 different stores

Our data consists of three parts:

1.  Transaction data
2.  List of ingredients --\> quality and quantity
3.  Metadata on restaurants

Forecast the daily demand for the next two weeks (from 16/06/2015 to 29/06/2015) for lettuce for store 46673. We also need to fill in the CSV file. All we have to do there is to substitute the numbers.

So the two steps of the project are:

-   Generate a time series of daily demand of lettuce --\> Most of the time

-   apply ARIMA and Holt-winters to predict the demand

-   Compare the results

### Importing the datasets

First, we import the relevant libraries to the project

```{r}
library(readr)
library(tidyr)
library(dplyr)
library(forecast)
library(ggplot2)
library(tseries)
```

We start by recognizing the schema of the assignment's dataset:

```{r message=FALSE, warning=FALSE}

# Import the main transactional table, which is "menu_items_all"

main <- read_csv('data/menuitem.csv')

# Columns that are necessary
columns <- c("MD5KEY_ORDERSALE", "StoreNumber", "PLU", "Id", "Quantity")

# Keep only necessary columns
main <- main[,columns]

# Filter for store of interest
main <- filter(main, StoreNumber == 46673)
```

Now we will join that table with the pos_ordersale table to get the date using the "MD5KEY_ORDERSALE" primary key

```{r message=FALSE, warning=FALSE}
# Import pos_ordersale table
pos_ordersale <- read_csv("data/pos_ordersale.csv")

main <- left_join(main, select(pos_ordersale, c("date", "MD5KEY_ORDERSALE")), by="MD5KEY_ORDERSALE")
```

Next, we want to bring in the recipe id. To do that, we will need to join the "menu_items" table on "PLU" and "Id" (left) and "RecipeId" (right)

```{r message=FALSE, warning=FALSE}
# Import pos_ordersale table
menu_items <- read_csv("data/menu_items.csv")

main <- left_join(main, select(menu_items, c("PLU", "MenuItemId", "RecipeId")), by = c("PLU" = "PLU", "Id" = "MenuItemId"))
```

```{r}
main
```

Our main transaction table is ready. Now, we need to figure out the quantity of lettuce per recipe. Then, we will be able to group the main table by date and have a total of lettuce demand for each day. Let's start by identifying the id of lettuce in the dataset

```{r}
# Grab lettuce's id
filter(ingredients, grepl("Lettuce", IngredientName, ignore.case = TRUE))
```

We see that there are two lettuces in the dataset. We know from the live session that the metric lettuce is not used in any of the transactions. Therefore, we will take 27 as the only id for lettuce.

Next, we import the recipes dataset

```{r message=FALSE, warning=FALSE}

# Importing recipes dataset

recipes <- read_csv("data/recipes.csv")
```

Then, we know that the ingredients can be used in an item in two ways: Via the ingredients directly or the sub-recipe. We will start by adding the lettuce quantities that come directly from the ingredients table.

```{r message=FALSE, warning=FALSE}

# Importing recipe_ingredients

recipe_ingredient_assignments <- read_csv("data/recipe_ingredient_assignments.csv")
# Filter only for lettuce

recipe_ingredient_assignments <- filter(recipe_ingredient_assignments, 
                                        IngredientId == 27)

# Join in the recipe table
recipes <- left_join(recipes, select(recipe_ingredient_assignments,
                                   c("RecipeId", "Quantity")), 
                    by="RecipeId") %>%
# Mutate to change the NA's to 0
           mutate(Quantity = ifelse(is.na(Quantity), 0, Quantity)) %>%
#Change the name of the column to specify it comes from direct  
           rename(quantity_from_direct = Quantity)
```

Now, we need to get the quantity of lettuce from the sub-recipes

```{r message=FALSE, warning=FALSE}

# Import sub recipe assignment dataset
recipe_sub_recipe_assignments <- read_csv("data/recipe_sub_recipe_assignments.csv")

# Import the sub_recipes ingredients assignment
sub_recipe_ingr_assignments <- read_csv("data/sub_recipe_ingr_assignments.csv")

# Join the sub_recipes with ingredients to get the recipe id
recipe_sub_recipe_joint <- left_join(sub_recipe_ingr_assignments,
                                     recipe_sub_recipe_assignments, 
                                     by='SubRecipeId')

# Filter for lettuce
recipe_sub_recipe_joint <- filter(recipe_sub_recipe_joint,
                                  IngredientId == 27) %>%
                           mutate(total_quantity = Factor*Quantity)

# Group by recipe
recipe_sub_recipe_joint <- recipe_sub_recipe_joint %>%
  group_by(RecipeId) %>%
  summarise(total_quantity = sum(total_quantity, na.rm = TRUE))

# Join with the recipes table
recipes_final <- left_join(recipes, recipe_sub_recipe_joint, by="RecipeId") %>%
# Mutate to change the NA's to 0
           mutate(total_quantity = ifelse(is.na(total_quantity), 
                                          0, total_quantity)) %>%
#Change the name of the column to specify it comes from direct  
           rename(quantity_from_sub = total_quantity) %>%
#Sum the two columns to get the total of lettuce
           mutate(total_lettuce = quantity_from_direct + quantity_from_sub) %>%
#Filter out recipes that do not have lettuce
           filter(total_lettuce > 0)
```

Then, we join our main transactional table again with this ingredietns adjusted table:

```{r message=FALSE, warning=FALSE}

# Join the main transactional table with recipes to get the final table
final <- left_join(main, select(recipes_final,
                                     c("RecipeId", "total_lettuce")), 
                        by="RecipeId") %>%
# Filter out NA's
              filter(!is.na(total_lettuce)) %>%
# Multiply by total quantity
              mutate(total_lettuce = Quantity*total_lettuce) %>%
# Group by date
              group_by(date) %>%
              summarise(total_lettuce = sum(total_lettuce))
final
```

The final step is to create a time-series out of it:

```{r}
# Transforming date field from the final table
final$date<- as.Date(paste0("20", substr(final$date, 1, 2), "-", substr(final$date, 4, 5), "-", substr(final$date, 7, 8)))

# Build time series
ts <- ts(final, start = min(final$date), frequency = 7)

# Plot the time series directly using ggplot2
ggplot(final, aes(date, total_lettuce)) + 
  geom_line() + 
  labs(title = "Time Series Plot of Lettuce Sales",
       x = "Date",
       y = "Total Lettuce Sales")

```

# Arima model

Next, we will use the ARIMA model to forecast the demand of lettuce for the next two weeks from the observation. In order to do that, we will apply the *Box-Jenkins* procedure to our dataset. It cosists of three steps [[ADD ARIMA MODEL DEFINITION HERE]]

1.  Identification
2.  Estimation
3.  Verification

## Identification

The first step is to identify if our data is stationary. A stationary time series is such that its statistical properties do not depend on the time at which the series is observed. Therefore, time series with trends or with seasonality, are not stationary and we need to remove them before we proceed with the application of the ARIMA model. In order to make a time series stationary we can either rescale the time series and apply differentiation to remove the deterministic components.

We start by making a simple plot of the time series as it is:

```{r}
library(scales)

# Plot the time series directly using ggplot2
ggplot(final, aes(date, total_lettuce)) + 
  geom_line() + 
  labs(title = "Time Series Plot of Lettuce Sales",
       x = "Date",
       y = "Total Lettuce Sales") + 
    scale_x_date(labels = date_format("%d-%b"), breaks = date_breaks("7 day")) + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```

There is no clear indication of trend nor deterministic error. However, there is some clear indication of seasonality. We can get a better idea of the individual components of the time series by plotting the decomposition series.

```{r}

# Transform our df into a time-series object
ar_ts <- ts(final[, 2], frequency=7, start = c(5,3))

# Plot the decomposition of the time-series 
stl_result <- decompose(ar_ts)

# Plotting stl
autoplot(stl_result) +
  labs(title = "STL Decomposition of Time Series",
       x = "Date",
       y = "Total Lettuce Sales")
```

As we suspected, there is no strong trend factor, but the size of the grey bar in the seasonal decompositon suggests there is seasonality. Moreover, we can say that the seasonality looks additive, that is, it's not changing with the series. We can apply the KPSS test for trend stationary. In this test the null hypothesis is that the time series is stationary. This means that a large p-value will mean we fail to reject the null and therefore the time series has no trend.

Before we proceed with testing and fitting, it is a good practice to split our data into a training and testing set. This will endure we will have a good way to compare our forecasts and assess the quality of our parameter choices. We will do a 75%-25% split

```{r}
# Splitting the dataset
train_data <- final[final$date <= as.Date('2015/05/20'),]
test_data <- final[final$date > as.Date('2015/05/20'),]

# Time series
train_ts <- ts(train_data[,2], frequency=7, start=c(05,03))
test_ts <- ts(test_data[,2], frequency=7, start=c(16,03))
final_ts <- ts(final[,2], frequency=7, start=c(05,03))
```

```{r}
autoplot(train_ts, series="Training") +
  autolayer(test_ts, series="Test") +
  guides(colour=guide_legend("Split")) +
  scale_color_manual(values=c("blue", "black")) +
  xlab("Date") +
  ylab("Lettuce consumption") + 
  ggtitle("Train/Test Split: Weekly Data")
```

We proceed to the KPSS test. This test test the time series specifically for trend stationarity, with the null hypothesis being that the time series is trend stationary.

```{r message=FALSE, warning=FALSE}

# Applying KPSS test
kpss.test(train_ts)
```

The p-value of the test was at least 0.1. So for a 10% confidence level, we fail to reject the hypothesis that the trend is stationary. With this is mind, we don't perform any differentiation on the time-series.

The next step is to deal with the seasonality factor. To this end, we will use the nsdiffs() function. This will give us the number of seasonal differences required to stationarize the time series, if necessary. We expect this function to return a value greater then 0, since we observe a seasonal trend on the plot.

```{r message=FALSE, warning=FALSE}

# Applying KPSS test
nsdiffs(train_ts)
```

The result of the function is 1. So we expect our best ARIMA model to have d=0 and D=1. Now we need to determine the p and q parameters of the model. To this end, we will analyse the correlation and partial correlation structures.

```{r message=FALSE, warning=FALSE}

# Applying KPSS test
ggtsdisplay(train_ts,
            plot.type='partial',
            main = 'ACF and PACF',
            smooth=TRUE)
```

The ACF plot shows clearly an autocorrelation at 7 and multiples of 7. This is expected for this dataset, since we expect our data to follow a weekly pattern, so the ACF confirms the seasonal behavior. On the PACF, we see that the plot fades after the lag 7. Therefore, a seasonal AR model with p = 7 seems to be the best choice. The following table shows the decision matrix for the ACF and PACF analysis.

|      | AR(p)                | MA(q)                | ARMA(p,q) |
|------|----------------------|----------------------|-----------|
| ACF  | Tails off            | Cust off after lag q | Tails off |
| PACF | Cust off after lag p | Tails off            | Tails off |

## Estimation

With this information, we will proceed to use auto.arima() to see if our expectations hold. We will use BIC (Bayes information criteria)

```{r message=FALSE, warning=FALSE}

# Applying the auto.arima function
auto.arima(train_ts, trace = TRUE, ic = 'bic', allowdrift = FALSE)
```

The auto.arima() function returned the best model to be ARIMA(0,0,0)(0,1,1)[7]. Recall that the ARIMA model is defined as: ARIMA(p,d,q)(P,D,Q)[s]. This means that p, d and q values refer to the non-seasonal AR. Therefore:

-   p = 0 means we need to account for the Y value at 0 lags from a given point t

-   d = 0 meains that there is no first-order differentiation needed (level stationary)

-   q = 0 means that the model the error term for the MA part should be taken from 0 lagged values

-   P = 0 means we don't need any seasonal auto-regressive numbers

-   D = 1 that we need one second order differentiation to remove the seasonality

-   Q = 1 refers to the number of seasonal moving average terms.

The next step is to use the Arima function to estimate the models based on the findings from the identification procedure. We take the three best models (i.e: lowest BIC values) in order to compare and validate their performance against the test data.

```{r message=FALSE, warning=FALSE}

# Devolping models for the top three models from the auto.arima() function

#ARIMA(0,0,0)(0,1,1)[7]
arima.m1 <- Arima(train_ts, order =c(0,0,0), 
                  seasonal = c(0,1,1), include.drift = FALSE)

#ARIMA(1,0,0)(0,1,1)[7]
arima.m2 <- Arima(train_ts, order =c(1,0,0), 
                  seasonal = c(0,1,1), include.drift = FALSE)

#ARIMA(0,0,0)(1,1,1)[7]
arima.m3 <- Arima(train_ts, order =c(0,0,1), 
                  seasonal = c(0,1,1), include.drift = FALSE)

```

We will use the three models above to forecast, and use the MSE against the test data to select the best performing one

```{r message=FALSE, warning=FALSE}

# Devolping models for the top three models from the auto.arima() function

# Forecast for ARIMA(0,0,0)(0,1,1)[7]
arima.f1 <- forecast(arima.m1, h=26)

#Forecast for ARIMA(1,0,0)(0,1,1)[7]
arima.f2 <- forecast(arima.m2, h=26)

#Forecast for ARIMA(0,0,1)(0,1,1)[7]
arima.f3 <- forecast(arima.m3, h=26)
```

```{r message=FALSE, warning=FALSE}

# Calculate accuracy for the first model
accuracy(arima.f1, test_ts)

# Calculate accuracy for the second model
accuracy(arima.f2, test_ts)

# Calculate accuracy for the third model
accuracy(arima.f3, test_ts)

```

Although we calculated with the auto.arima function the best model to be ARIMA(0,0,0)(0,1,1)[7], it turns out the variation ARIMA(0,0,0)(1,1,1)[7] performed better on the test set. The difference between these two models is that now the P term should be one, meaning we take 1 autoregressive number for the seasonal part of the ARIMA model

```{r message=FALSE, warning=FALSE}

best_arima.m3 <- Arima(final_ts, order=c(0,0,0), seasonal = c(1,1,1))

best_arima.m3.f <- forecast(best_arima.m3, h=14)

plot(best_arima.m3.f, main="ARIMA forecast for the next 14 days", xlab='Date',
     ylab='Lettuce demand (ounces)')
```

## Verification

Check if the model fits the data using residuals analysis

-   Calculate and plot the residuals from the model. The graph should give no indication of a non-zero mean or non-constant variance

-   Plot the sample ACF off the residuals. No more than two or three out of 40 shall fall outside the bounds \$/frac{1.96}{\\sqrt{n}}\$

-   The same applies to sample PACF of the residuals

-   Test for randomness of the residuals

```{r}
checkresiduals(best_arima.m3)
```

# Holt-Winters

For time series analysis, the first step is always to visually inspect the time series. In this regard, the stl() function is quite useful. The arguments of the ets function are the time series and the model to be used. We can pass "ZZZ" as the argument for the model to let the function decide on the best parameters for the training. Let's go back to the decomposition analysis.

```{r}
# Plotting stl
autoplot(stl_result) +
  labs(title = "STL Decomposition of Time Series",
       x = "Date",
       y = "Total Lettuce Sales")
```

From this analysis, we could expect the following parameters for the Holt-Winters model:

-   Trend: None

-   Seasonality: Additive. This is because, although there is clearly seasonality, it does not change pattern depending on the period t.

-   Error: Additive, i.e: We don't see any clear evidence of error changing with the period t.

Next, we run the ets function with the "ZZZ" parameter to see if our expectations are confirmed.

```{r}

train_ts.ets1 <- ets(train_ts, model = "ZZZ")

train_ts.ets1
```

It turns out that the ETS returned a model which is aligned with our expectations. Therefore, we won't train any additional models.

# Models comparison

We compare its performance agains the ARIMA models trained in the previous step:

```{r}
# Forecast the test set
hw.f1 <- forecast(train_ts.ets1,h=26)

# Determine accuracy against the test set
acc_4 <- accuracy(hw.f1, test_ts)

# Print accuracy
acc_4
```

Therefore,

```{r}
final_forecast <- data.frame(best_arima.m3.f$mean)
write.csv(final_forecast, "final_forecast.csv")
```
